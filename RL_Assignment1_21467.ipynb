{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamBhawsar2001/Reinforcement-learning-project/blob/main/RL_Assignment1_21467.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Maze:\n",
        "    def __init__(self, gridHeight=6, gridWidth=6, terminalReward=10, lockPickProb=0.5):\n",
        "        self.rewardsLeft = np.array([[-1, 0, 0, 0, 0, 0], \n",
        "                                    [-1, -1, 0, 0, 0,-10], \n",
        "                                    [-1, 0, 0, -1, -1, -1], \n",
        "                                    [0, 0, 0, -10, -1, -1],\n",
        "                                    [-1, -1, 0, 0, -1, 0],\n",
        "                                    [-1, 0, -1, 0, 0 ,-1]])\n",
        "\n",
        "        self.rewardsRight =  np.array([[ 0, 0, 0, 0, 0, -1], \n",
        "                            [ -1, 0, 0 , 0, -10, -1],\n",
        "                            [ 0, 0, -1, -1, -1, -1],\n",
        "                            [ 0, 0, -10, -1, -1 ,-1],\n",
        "                            [ -1, 0, 0, -1, 0, -1],\n",
        "                            [ 0, -1, 0, 0, -1, -1]])\n",
        "\n",
        "        self.rewardsUp  =  np.array([[ -1, -1, -1, -1, -1, -1], \n",
        "                            [ 0, -1, -1, -1, -1, 0],\n",
        "                            [ 0, 0, -1, 0, 0, 0],\n",
        "                            [ -1, 0,0, 0,0, 0],\n",
        "                            [ 0, -10, -1, -1, -1, 0],\n",
        "                            [ 0,  0, -1, -10, 0, 0]])\n",
        "\n",
        "\n",
        "        self.rewardsDown =  np.array([[ 0, -1, -1, -1, -1, 0], \n",
        "                            [ 0, 0, -1, 0, 0, 0],\n",
        "                            [ -1, 0, 0, 0, 0, 0],\n",
        "                            [ 0, -10,-1,-1,-1, 0],\n",
        "                            [  0,0,-1,-10,0, 0],\n",
        "                            [ -1, -1, -1, 0, -1, -1]])\n",
        "\n",
        "        self.gridHeight = gridHeight\n",
        "        self.gridWidth = gridWidth\n",
        "        self.lockPickProb = lockPickProb\n",
        "        self.terminalReward = terminalReward\n",
        "\n",
        "\n",
        "    def isStateTerminal(self, state):\n",
        "        if state == (3, 0) :\n",
        "            return True\n",
        "        elif state == (5, 3):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def takeAction(self, state, action):\n",
        "        retVal = []\n",
        "        if(self.isStateTerminal(state)):\n",
        "            return [[state,1, self.terminalReward]] \n",
        "\n",
        "        if action=='left':\n",
        "            reward = self.rewardsLeft[state]\n",
        "            if(reward == -1):\n",
        "                retVal.append([state,1,-1])\n",
        "            elif(reward == -10):\n",
        "                retVal.append([(state[0], state[1]-1),self.lockPickProb,-1])\n",
        "                retVal.append([state,1-self.lockPickProb,-1])\n",
        "            else:\n",
        "                retVal.append([(state[0], state[1]-1),1,-1])\n",
        "\n",
        "        if action=='right':\n",
        "            reward = self.rewardsRight[state]\n",
        "            if(reward == -1):\n",
        "                retVal.append([state,1,-1])\n",
        "            elif(reward == -10):\n",
        "                retVal.append([(state[0], state[1]+1),self.lockPickProb,-1])\n",
        "                retVal.append([state,1-self.lockPickProb,-1])\n",
        "            else:\n",
        "                retVal.append([(state[0], state[1]+1),1,-1])\n",
        "\n",
        "        if action=='up':\n",
        "            reward = self.rewardsUp[state]\n",
        "            if(reward == -1):\n",
        "                retVal.append([state,1,-1])\n",
        "            elif(reward == -10):\n",
        "                retVal.append([(state[0]-1, state[1]),self.lockPickProb,-1])\n",
        "                retVal.append([state,1-self.lockPickProb,-1])\n",
        "            else:\n",
        "                retVal.append([(state[0]-1, state[1]),1,-1])\n",
        "\n",
        "        if action=='down':\n",
        "            reward = self.rewardsDown[state]\n",
        "            if(reward == -1):\n",
        "                retVal.append([state,1,-1])\n",
        "            elif(reward == -10):\n",
        "                retVal.append([(state[0]+1, state[1]),self.lockPickProb,-1])\n",
        "                retVal.append([state,1-self.lockPickProb,-1])\n",
        "            else:\n",
        "                retVal.append([(state[0]+1, state[1]),1,-1])\n",
        "        for i,[nextState, prob, reward] in enumerate(retVal):\n",
        "            if(self.isStateTerminal(nextState)):\n",
        "                retVal[i][2] = self.terminalReward   \n",
        "\n",
        "        return retVal \n",
        "    \n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "rvLs3SkGWY3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridworldSolution:\n",
        "    def __init__(self, maze,horizonLength):\n",
        "        self.env = maze\n",
        "        self.actionSpace = ['left', 'right', 'up',  'down']\n",
        "        self.horizonLength = horizonLength\n",
        "        self.DP = np.ones((self.env.gridHeight,self.env.gridWidth,self.horizonLength),dtype = float) * -np.inf\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "    def optimalReward(self, state, k):\n",
        "        optReward = -np.inf\n",
        "        dp=np.matrix(np.ones((6,6)) * (-np.inf))\n",
        "        dp[3,0]=20\n",
        "        dp[5,3]=20\n",
        "        def dpsol(self,state,k,dp,action):\n",
        "          l=maze.takeAction(state,'left')\n",
        "          lv=-np.inf\n",
        "          r= maze.takeAction(state,'right')\n",
        "          rv=-np.inf\n",
        "          u= maze.takeAction(state,'up')\n",
        "          uv=-np.inf\n",
        "          d= maze.takeAction(state,'down')\n",
        "          dv=-np.inf\n",
        "          \n",
        "          if  dp[state[0],state[1]]!=(-np.inf):\n",
        "            return dp[state[0],state[1]]\n",
        "          \n",
        "          if(k>100):\n",
        "            return -np.inf\n",
        "          if state!=l[0][0] and (action!='right'):\n",
        "           lv= l[0][2]+l[0][1]*dpsol(self,l[0][0],k+1,dp,'left')\n",
        "          if state!=r[0][0] and (action!='left'):\n",
        "           rv= r[0][2]+r[0][1]*dpsol(self,r[0][0],k+1,dp,'right')\n",
        "          if state!=u[0][0] and (action!='down'):\n",
        "           uv= u[0][2]+u[0][1]*dpsol(self,u[0][0],k+1,dp,'up')\n",
        "          if state!=d[0][0] and (action!='up'):\n",
        "           dv= d[0][2]+d[0][1]*dpsol(self,d[0][0],k+1,dp,'down')\n",
        "          \n",
        "          dp[state[0],state[1]]=max(lv,rv,uv,dv)\n",
        "          return dp[state[0],state[1]]\n",
        "        \n",
        "        optReward=dpsol(self,state,k,dp,'any')\n",
        "        #### Write your code here\n",
        "        \n",
        "\n",
        "        print(dp)\n",
        "        ########\n",
        "        return optReward\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    maze = Maze()\n",
        "    solution = GridworldSolution(maze,horizonLength=5)\n",
        "    print(\" Horizon \",solution.horizonLength)\n",
        "    optReward = solution.optimalReward((2,0),0)\n",
        "    print(optReward)\n",
        "    assert optReward==28.0, 'wrong answer'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cUXdWaGWdT3",
        "outputId": "499571b7-f467-43ca-b7e0-de075af8f5c8"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Horizon  5\n",
            "[[ 19.  20.  21.  22.  23.  24.]\n",
            " [ 18.  28.  27.  26.  25.  25.]\n",
            " [ 28.  29.  28.  23. -inf  26.]\n",
            " [ 20.  30.  29.  22. -inf  27.]\n",
            " [ 30.  27.  19.  20.  29.  28.]\n",
            " [ 29.  28. -inf  20.  30. -inf]]\n",
            "28.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": null
    },
    "microsoft": {
      "language": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}